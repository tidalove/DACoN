{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322708e7-7463-4e5d-be4e-d1976481176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f839ef-d148-415a-9483-5bdf09c0a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecdbd04c-47f6-4cd1-ac13-20bf7dd4d0b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'krita_dacon_single_pad_collate_fn' from 'data' (/orcd/home/002/kav/DACoN/dacon/data/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DACoNModel, DACoNTinyModel\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KritaDACoNSingleDataset, krita_dacon_single_pad_collate_fn\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     move_data_to_device,\n\u001b[32m      5\u001b[39m     load_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     make_krita_inference_data_list,\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'krita_dacon_single_pad_collate_fn' from 'data' (/orcd/home/002/kav/DACoN/dacon/data/__init__.py)"
     ]
    }
   ],
   "source": [
    "from models import DACoNModel, DACoNTinyModel\n",
    "from data import KritaDACoNSingleDataset, krita_dacon_single_pad_collate_fn\n",
    "from utils import (\n",
    "    move_data_to_device,\n",
    "    load_config,\n",
    "    format_time,\n",
    "    colorize_target_image,\n",
    "    get_folder_names,\n",
    "    get_file_names,\n",
    "    extract_segment,\n",
    "    extract_color,\n",
    "    make_krita_inference_data_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854de6e6-e3b2-4112-b77f-7ba2a93b32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_seg_and_color(data_root, line_name, color_name):\n",
    "\n",
    "    # all frames have corresponding lines - get their basenames from lines\n",
    "    frame_names = get_file_names(os.path.join(data_root, line_name, \"target\"))\n",
    "    ref_frame_names = get_file_names(os.path.join(data_root, line_name, \"ref\"))\n",
    "\n",
    "    for frame_name in frame_names:\n",
    "        line_image_path = os.path.join(data_root, line_name, \"target\", frame_name)\n",
    "        seg_path = os.path.join(data_root, line_name, \"seg\", frame_name)\n",
    "        os.makedirs(os.path.join(data_root, line_name, \"seg\"), exist_ok=True)\n",
    "\n",
    "        if not(os.path.isfile(seg_path)):\n",
    "            extract_segment(line_image_path, seg_path)\n",
    "\n",
    "    for frame_name in ref_frame_names:\n",
    "        color_image_path = os.path.join(data_root, color_name, \"ref\", frame_name)\n",
    "        line_image_path = os.path.join(data_root, line_name, \"ref\", frame_name)\n",
    "        seg_path = os.path.join(data_root, line_name, \"seg\", \"ref\", frame_name)\n",
    "        color_json_path = os.path.join(data_root, line_name, \"seg\", f\"{frame_name.split('.')[0]}.json\")\n",
    "        os.makedirs(os.path.join(data_root, line_name, \"seg\", \"ref\"), exist_ok=True)\n",
    "\n",
    "        if not(os.path.isfile(seg_path)):\n",
    "            extract_segment(line_image_path, seg_path)\n",
    "        if not(os.path.isfile(color_json_path)):\n",
    "            extract_color(color_image_path, seg_path, color_json_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb949e1d-7451-4d29-85e6-f1ea39899158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_inference(line_name, color_name,\n",
    "                  config='../configs/krita-inference.yaml',\n",
    "                  model_path='../checkpoints/dacon_krita.pth',\n",
    "                  data_root='./tmp'):\n",
    "\n",
    "    config = load_config(config)\n",
    "\n",
    "    version = config['version']\n",
    "    num_workers_val = config['datasets']['val']['num_worker']\n",
    "\n",
    "    batch_size = 1\n",
    "    save_images = config['val']['save_images']\n",
    "    save_json = config['val']['save_json']\n",
    "    save_path = data_root\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and config['num_gpu'] > 0 else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = DACoNTinyModel(config['network'], version).to(device)\n",
    "\n",
    "    print(f\"Loading checkpoint {os.path.basename(model_path)}\")\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    \n",
    "    # --- Dataset and DataLoader setup ---\n",
    "    print(f\"Extracting Segment and Color\")\n",
    "    check_seg_and_color(data_root, line_name, color_name)\n",
    "\n",
    "    print(\"\\n--- Start Inference ---\")\n",
    "    inference_start_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ref_data_list = make_krita_inference_data_list(data_root, line_name, color_name, is_ref = True)\n",
    "        ref_dataset = KritaDACoNSingleDataset(ref_data_list, data_root, is_ref=True, mode = \"infer\")\n",
    "        ref_dataloader = DataLoader(ref_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_val, collate_fn=dacon_single_pad_collate_fn)\n",
    "\n",
    "        all_seg_feats_ref = torch.empty(0, device=device)\n",
    "        all_seg_colors_ref = torch.empty(0, device=device)\n",
    "\n",
    "        for i, ref_data in enumerate(ref_dataloader):\n",
    "            ref_data = move_data_to_device(ref_data, device)\n",
    "            seg_colors_ref = ref_data['seg_colors']\n",
    "            seg_feats_ref, _ = model._process_single(ref_data['line_image'], ref_data['seg_image'], ref_data[\"seg_num\"])\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                all_seg_feats_ref = torch.cat((all_seg_feats_ref, seg_feats_ref[b]), dim = 0)\n",
    "                all_seg_colors_ref = torch.cat((all_seg_colors_ref, seg_colors_ref[b]), dim = 0)\n",
    "\n",
    "            del ref_data, seg_feats_ref\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        inference_data_list = make_krita_inference_data_list(data_root, line_name, color_name, is_ref = False)\n",
    "        inference_dataset = KritaDACoNSingleDataset(inference_data_list, data_root, is_ref=False, mode = \"infer\")\n",
    "        inference_dataloader = DataLoader(inference_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_val, collate_fn=krita_dacon_single_pad_collate_fn)\n",
    "\n",
    "        print(f\"\\n  Inference on {len(inference_dataset)} samples from ref on {color_name}\")\n",
    "\n",
    "        all_seg_feats_ref = all_seg_feats_ref.unsqueeze(0)\n",
    "        all_seg_feats_ref = all_seg_feats_ref.repeat(batch_size, 1, 1)\n",
    "\n",
    "        for i, data in enumerate(inference_dataloader):\n",
    "            data = move_data_to_device(data, device)\n",
    "            seg_feats_tgt, _ = model._process_single(data['line_image'], data['seg_image'], data[\"seg_num\"])\n",
    "            seg_sim_map = model.get_seg_cos_sim(all_seg_feats_ref.unsqueeze(1), seg_feats_tgt.unsqueeze(1))\n",
    "            seg_sim_map = seg_sim_map.squeeze(1)\n",
    "\n",
    "            frame_name = data[\"frame_name\"]\n",
    "            line_image_tgt = data[\"line_image\"] \n",
    "            seg_image_tgt = data[\"seg_image\"]\n",
    "            color_name = data[\"color_name\"]\n",
    "            line_name = data[\"line_name\"]\n",
    "\n",
    "            for b in range(batch_size):\n",
    "\n",
    "                seg_sim_map_batch = seg_sim_map[b]\n",
    "                nearest_patch_indices = torch.argmax(seg_sim_map_batch, dim=-1)\n",
    "\n",
    "                color_list_pred = all_seg_colors_ref[nearest_patch_indices]\n",
    "                color_list_pred = color_list_pred * 255\n",
    "\n",
    "                if save_images:\n",
    "                    image_pred = colorize_target_image(color_list_pred, line_image_tgt[b], seg_image_tgt[b], colors_only=True)\n",
    "                    folder_path = os.path.join(save_path, color_name[b], \"pred\")\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "                    file_path  = os.path.join(folder_path, f\"{frame_name[b]}.png\")\n",
    "                    image_pred = image_pred.permute(2, 0, 1)\n",
    "                    save_image(image_pred, file_path)\n",
    "\n",
    "                if save_json:\n",
    "                    folder_path = os.path.join(save_path, color_name[b], \"pred\")\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "                    json_file_path  = os.path.join(folder_path, f\"{frame_name[b]}.json\")\n",
    "                    color_dict = {str(idx + 1): [int(value) for value in color.tolist()] for idx, color in enumerate(color_list_pred)}\n",
    "\n",
    "                    with open(json_file_path, \"w\") as json_file:\n",
    "                        json.dump(color_dict, json_file)\n",
    "\n",
    "            print(f\"  Sample {i+1}/{len(inference_dataset)}\", end='\\r')\n",
    "\n",
    "        del data, seg_feats_tgt, seg_sim_map\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del all_seg_feats_ref\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n--- Inference complete! ---\")\n",
    "    inference_finish_time = time.time()\n",
    "    print(f\"\\nTotal time: {format_time(inference_finish_time - inference_start_time)}\")\n",
    "\n",
    "    # Log absolute peak memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory_gb = torch.cuda.max_memory_allocated() / 1024**3\n",
    "        print(f\"Peak GPU Memory Usage: {peak_memory_gb:.2f}GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65fd7e30-3018-4173-af1f-31df18021fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading checkpoint dacon_krita.pth\n",
      "Extracting Segment and Color\n",
      "\n",
      "--- Start Inference ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kav/.conda/envs/dacon/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kav/.conda/envs/dacon/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/orcd/home/002/kav/DACoN/dacon/data/dataloader.py\", line 58, in dacon_single_pad_collate_fn\n    char_name = [item['char_name'] for item in batch]\n                 ~~~~^^^^^^^^^^^^^\nKeyError: 'char_name'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlines\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcolors\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mrun_inference\u001b[39m\u001b[34m(line_name, color_name, config, model_path, data_root)\u001b[39m\n\u001b[32m     41\u001b[39m all_seg_feats_ref = torch.empty(\u001b[32m0\u001b[39m, device=device)\n\u001b[32m     42\u001b[39m all_seg_colors_ref = torch.empty(\u001b[32m0\u001b[39m, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mref_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove_data_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseg_colors_ref\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mseg_colors\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dacon/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dacon/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1506\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1504\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1505\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dacon/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1541\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1539\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dacon/lib/python3.12/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mKeyError\u001b[39m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kav/.conda/envs/dacon/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kav/.conda/envs/dacon/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/orcd/home/002/kav/DACoN/dacon/data/dataloader.py\", line 58, in dacon_single_pad_collate_fn\n    char_name = [item['char_name'] for item in batch]\n                 ~~~~^^^^^^^^^^^^^\nKeyError: 'char_name'\n"
     ]
    }
   ],
   "source": [
    "run_inference('lines','colors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2260f9b1-09f1-4d14-a9e2-4303f4ccb3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colors\tlines\n"
     ]
    }
   ],
   "source": [
    "!ls ./tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e770a07-5e95-4750-97f2-2755eaefb460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dacon)",
   "language": "python",
   "name": "dacon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
